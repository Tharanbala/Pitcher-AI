{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfb2eXfcSAg6Im+qICFAWM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharanbala/product-reviewer/blob/tirth_backend/async_agent_voting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JeLfwCvgVn7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98f675a-e98c-492b-f88f-39c9a85e0364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n",
            "Collecting groq\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
            "Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, groq\n",
            "Successfully installed groq-0.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0\n",
            "Collecting openai\n",
            "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, openai\n",
            "Successfully installed jiter-0.5.0 openai-1.42.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv\n",
        "!pip install groq\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIoS9bUwWY-O",
        "outputId": "e8e425af-26ae-4b09-988a-a45633176e72"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"model_responses\": [\n",
        "        {\n",
        "            \"model\": \"llama3-8b-8192\",\n",
        "            \"completion\": \"Hi guys!\\n\\nAre you tired of breaking the bank on a new TV? Look no further! I'm excited to share with you three incredible options from TCL that won't burn a hole in your pocket.\\n\\nFirst up, we have the TCL 55\\\" Class 4-Series 4K UHD Smart Roku TV, which is currently priced at just $363.01 on Amazon. This TV boasts stunning 4K picture quality with excellent brightness and contrast, making it perfect for immersing yourself in your favorite movies and shows. Plus, it's equipped with all major streaming apps, including HBO Max, and features a user-friendly Roku OS.\\n\\nIf budget is a major concern, the TCL 55\\\" Class 4-Series LED 4K UHD Smart Android TV is another fantastic option. You can grab this one for around $200 on Best Buy, which is an absolute steal! This TV offers excellent picture quality, great value for its price, and all the major streaming apps. However, keep in mind that the remote control and sound quality might not be as impressive as some of the higher-end models.\\n\\nFor those looking to splurge a bit more, the TCL 55\\\" 4K UHD Smart TV (55R646) is a premium option that's worth considering. With a price tag of around $949.99, you get strong performance, excellent contrast and color accuracy, and impressive features like a 120Hz refresh rate and VRR for gaming. However, some users have reported software instability issues, so be sure to check for updates before making the purchase.\\n\\nIn conclusion, the TCL 55\\\" Class 4-Series 4K UHD Smart Roku TV and TCL 55\\\" Class 4-Series LED 4K UHD Smart Android TV are both excellent budget options that won't compromise on picture quality. The TCL 55\\\" 4K UHD Smart TV (55R646) is a more premium model that offers strong performance and features, but be aware of the potential software issues.\\n\\nWhich one will you choose? Let me know in the comments below!\\n\\nTime's up! Thanks for watching, and I'll see you in the next video!\"\n",
        "        },\n",
        "        {\n",
        "            \"model\": \"mixtral-8x7b-32768\",\n",
        "            \"completion\": \"Hey there, millennials! I'm here to talk about the TCL 55\\\" Class 4-Series 4K UHD Smart Roku TV (55S455) that you can snag on Amazon for just $363.01. This TV is a budget-friendly option that packs a punch when it comes to picture quality and value for money.\\n\\nWith stunning 4K Ultra HD and excellent brightness and contrast, you'll be blown away by the picture on this 55-inch screen. Plus, it has all the major streaming apps, including HBO Max, making it easy for you to access your favorite shows and movies. And, the Roku OS is super easy to navigate, so you can spend less time figuring out how to use your TV and more time enjoying it.\\n\\nNow, let's talk about the cons. Some users find the remote control to be a little less user-friendly than other models, and the built-in speakers may not provide the best sound quality for those who prefer something more substantial. However, with a 4.4 out of 5-star rating on Best Buy, many users praise its picture quality and value for money.\\n\\nIf you're looking for an even more affordable option, check out the TCL 55\\\" Class 4-Series LED 4K UHD Smart Android TV (55S434) available on Best Buy for approximately $200. It has excellent picture quality and brightness, making it a fantastic budget option. The con is the remote control, but other than that, it's a solid choice.\\n\\nFinally, if you're willing to splurge, the TCL 55\\\" 4K UHD Smart TV (55R646) offers strong performance with excellent contrast, color accuracy, and all major streaming apps, including Google Cast and hands-free Google Assistant. The downside is that some users have reported software instability, but TCL has released updates to address this issue.\\n\\nIn conclusion, the TCL 55\\\" Class 4-Series 4K UHD Smart Roku TV and the TCL 55\\\" Class 4-Series LED 4K UHD Smart Android TV are both excellent budget options. The TCL 55\\\" 4K UHD Smart TV (55R646) is a premium model with strong performance and features, but it has had some issues with software stability. Whatever you choose, you can't go wrong with TCL!\"\n",
        "        },\n",
        "        {\n",
        "            \"model\": \"gemma-7b-it\",\n",
        "            \"completion\": \"## Content Creator Social Media Pitch: TCL 55\\\" 4K UHD Smart TVs\\n\\n**Hey there, content creators!** 📺\\n\\nEver wanted to upgrade your entertainment game without breaking the bank? Look no further than the amazing TCL 55\\\" 4K UHD Smart TVs! 🤯\\n\\n**Here's what you need to know:**\\n\\n**✅ Budget-friendly bliss:** Get incredible 4K picture and fantastic brightness for under $363! 🤯\\n**✅ Entertainment central:** Enjoy all your favorite streaming apps like Netflix, Hulu, and HBO Max right out of the box. 🍿\\n**✅ Smart as can be:** The Roku OS is super easy to navigate and control. 🔥\\n\\n**But that's not all!** There are even more reasons to love these TCL TVs:\\n\\n- **Excellent contrast and color accuracy** for super realistic picture quality. 🌈\\n- **Dedicated gaming features** like 120Hz refresh rate and VRR for next-level gaming. 🎮\\n- **Free software updates** to keep things fresh and secure. 🛡️\\n\\n**And get this:** TCL offers two different models to fit your needs and budget. 🤑\\n\\n- **TCL 55\\\" 4K UHD Smart Roku TV:** Great for those who want a budget-friendly 4K TV with fantastic picture quality. \\n- **TCL 55\\\" 4K UHD Smart Android TV:** Offers more features and Google Assistant for a slightly higher price.\\n\\n**So why wait?** Upgrade your entertainment experience with a TCL 55\\\" 4K UHD Smart TV today and start streaming, gaming, and enjoying your favorite shows in stunning detail! 🤩\\n\\n**Click the link in my bio to learn more and snag an awesome deal!** ➡️ [Insert Link]\\n\\n**Don't forget to tag @TCLTV in your posts and use the hashtag #TCLLife for a chance to be featured!** ✨\\n\\n**#TVReview #SmartTV #BudgetGaming #EntertainmentBoost #TCLLife**\"\n",
        "        },\n",
        "        {\n",
        "            \"model\": \"gpt_4\",\n",
        "            \"completion\": \"Friends, welcome back! Life is all about experiences, right? And while we’re at it, why not experience the best in visual entertainment? I have three stunning products here that are transforming couch-potato life as we know it. Shout-out to all those 4K enthusiasts and streamaholics out there, today we're talking about some breakthrough TVs from TCL!\\n\\nFirst up, TCL's 55” Class 4-Series 4K UHD Smart Roku TV. Priced at an attractive $363.01 with an impressive 4.4 stars rating to its credit, this smart TV is a contemporary classic. Ultra HD picture quality, top streaming apps, and an easy-to-use Roku interface - it's got it all. It might not have the best remote control or built-in speakers, but this creature of value is an undeniable game changer - all that for the price of regular dinner outings!\\n\\nNext, we have TCL's other variant, the 55\\\" Class 4-Series LED 4K UHD Smart Android TV. Priced at around $200, this gem is an absolute steal! The product offers an incredible balance between picture quality and affordability—a true testimony to TCL's innovation. Same pros, same gripes with the remote and speakers, but boy, that price point is just too irresistible!\\n\\nLastly, for the perfectionists out there, we've got a premium treat: TCL 55\\\" 4K UHD Smart TV (55R646). This higher-end 55\\\" beauty not only provides excellent contrast and color accuracy, but also unique gaming features with a 120Hz refresh rate, VRR support, and a full Google ecosystem. Priced at $949.99, it's for those unwilling to compromise on vision. Although plagued by some software stability issues in the past, it seems like TCL is addressing these diligently.\\n\\nSo folks, today I talked about three products that prove quality does not necessarily drive up prices. Whether you're a value buyer or a gaming enthusiast, I hope we've thrown some light on TCL's range that's been disrupting the 4K market. As always, pick what works best for your needs and budget. Until next time - keep streaming, keep experiencing!\"\n",
        "        }\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "Wzok07QGWGCH"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_category = (25, 35)"
      ],
      "metadata": {
        "id": "prium3VqWJJN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROFESSIONS = [\"teacher\", \"barber\", \"dancer\", \"engineer\", \"doctor\", \"artist\", \"chef\", \"writer\", \"nurse\", \"lawyer\", \"musician\", \"actor\", \"scientist\", \"pilot\",\"student\"]\n"
      ],
      "metadata": {
        "id": "iYclSZihWS6H"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import asyncio\n",
        "from groq import AsyncGroq\n",
        "import nest_asyncio\n",
        "from openai import AsyncClient"
      ],
      "metadata": {
        "id": "tWzd4SsWWey3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groq_client_1 = AsyncGroq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "groq_client_2 = AsyncGroq(api_key=os.getenv(\"GROQ_API_KEY_TIRTH\"))\n",
        "\n",
        "openai_client = AsyncClient(api_key=os.getenv(\"OPENAI_API_KEY_Tirth\"))"
      ],
      "metadata": {
        "id": "tbPiT0ICWmau"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "FUfumzQ_W3a2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "async def get_response_with_retry(client, model, pitch, age, profession, max_retries=3):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            response = await client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": f\"You are a {age}-year-old {profession}. The following is a marketing pitch: '{pitch}'. Would you buy this product? Please respond with 'Yes' or 'No' and explain why.\"\n",
        "                    }\n",
        "                ],\n",
        "                temperature=1,\n",
        "                max_tokens=50,\n",
        "                top_p=1,\n",
        "                stream=False\n",
        "            )\n",
        "            text_response = response.choices[0].message.content.strip().lower()\n",
        "            return \"yes\" in text_response\n",
        "        except Exception as e:\n",
        "            if \"503\" in str(e):\n",
        "                retries += 1\n",
        "                wait_time = 2 ** retries + random.uniform(0, 1)  # Exponential backoff\n",
        "                print(f\"Retrying in {wait_time:.2f} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "    raise Exception(\"Max retries exceeded. Service is unavailable.\")\n",
        "\n",
        "# Use this function in your get_audience_responses method\n"
      ],
      "metadata": {
        "id": "JA0GPkH4kf70"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def get_response_from_llm(client, model, pitch, age, profession):\n",
        "    response = await client.chat.completions.create(\n",
        "        model= model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"You are a {age}-year-old {profession}. The following is a marketing pitch: '{pitch}'. Would you buy this product? Please respond with 'Yes' or 'No' and explain why.\"\n",
        "            }\n",
        "        ],\n",
        "        temperature=1,\n",
        "        max_tokens=50,\n",
        "        top_p=1,\n",
        "        stream=False\n",
        "    )\n",
        "    text_response = response.choices[0].message.content.strip().lower()\n",
        "    return \"yes\" in text_response"
      ],
      "metadata": {
        "id": "32A6qe7OXkjW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create an audience member\n",
        "def create_audience_member(age_category, professions, count):\n",
        "    age = random.randint(age_category[0], age_category[1])\n",
        "    profession = random.choice(professions)\n",
        "    if(count%2 == 0):\n",
        "      llm = random.choice(['mixtral-8x7b-32768', 'llama-3.1-70b-versatile', 'llama3-70b-8192', 'llama-guard-3-8b','gpt-4o'])\n",
        "    else:\n",
        "      llm = random.choice(['llama3-8b-8192', 'gemma-7b-it', 'llama-3.1-8b-instant', 'gemma2-9b-it','gpt-4o'])\n",
        "    return {\n",
        "        \"age\": age,\n",
        "        \"profession\": profession,\n",
        "        \"llm\": llm\n",
        "    }"
      ],
      "metadata": {
        "id": "WnqFhvcRYdBO"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the audience\n",
        "def create_audience(num_people, age_category, professions):\n",
        "    return [create_audience_member(age_category, professions, i) for i in range(num_people)]"
      ],
      "metadata": {
        "id": "QaDv5sScYxkG"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_list_1 =['llama3-70b-8192', 'llama-guard-3-8b']\n",
        "llm_list_2 = ['llama-3.1-8b-instant', 'gemma2-9b-it']\n",
        "llm_list_3 = ['mixtral-8x7b-32768', 'llama-3.1-70b-versatile']\n",
        "llm_list_4 =['llama3-8b-8192', 'gemma-7b-it']"
      ],
      "metadata": {
        "id": "t_AMtpmIPUrP"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Get the response from the audience\n",
        "async def get_audience_responses(audience, pitch):\n",
        "    tasks = []\n",
        "\n",
        "    for person in audience:\n",
        "        age = person['age']\n",
        "        profession = person['profession']\n",
        "        llm = person['llm']\n",
        "\n",
        "        # Choose the correct client based on the LLM\n",
        "        if llm == 'gpt-4o':\n",
        "            tasks.append(get_response_from_llm(AsyncClient(api_key=os.getenv(\"OPENAI_API_KEY_Tirth\")), llm, pitch, age, profession))\n",
        "        elif llm in llm_list_1:\n",
        "            tasks.append(get_response_from_llm(AsyncGroq(api_key=os.getenv(\"GROQ_API_KEY\")), llm, pitch, age, profession))\n",
        "        elif llm in llm_list_2:\n",
        "            tasks.append(get_response_from_llm(AsyncGroq(api_key=os.getenv(\"GROQ_API_KEY2\")), llm, pitch, age, profession))\n",
        "        elif llm in llm_list_3:\n",
        "            tasks.append(get_response_from_llm(AsyncGroq(api_key=os.getenv(\"GROQ_API_KEY_TIRTH_2\")), llm, pitch, age, profession))\n",
        "        else:\n",
        "            tasks.append(get_response_from_llm(AsyncGroq(api_key=os.getenv(\"GROQ_API_KEY_TIRTH\")), llm, pitch, age, profession))\n",
        "    pending_tasks = [task for task in asyncio.all_tasks() if not task.done()]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Combine the results with audience details\n",
        "    responses = [dict(person, response=result) for person, result in zip(audience, results)]\n",
        "    return responses"
      ],
      "metadata": {
        "id": "mpi7sejHY4t0"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audience = create_audience(100, age_category, PROFESSIONS)"
      ],
      "metadata": {
        "id": "GqSmhqP0aGzU"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(audience)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfzUBm1icLi6",
        "outputId": "8b3b5dd1-4b87-4a4b-904a-e5ac0a10fdf9"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'age': 26, 'profession': 'chef', 'llm': 'llama-3.1-70b-versatile'}, {'age': 34, 'profession': 'chef', 'llm': 'gpt-4o'}, {'age': 29, 'profession': 'chef', 'llm': 'llama-3.1-70b-versatile'}, {'age': 28, 'profession': 'writer', 'llm': 'llama-3.1-8b-instant'}, {'age': 26, 'profession': 'chef', 'llm': 'llama-guard-3-8b'}, {'age': 34, 'profession': 'actor', 'llm': 'gpt-4o'}, {'age': 25, 'profession': 'actor', 'llm': 'mixtral-8x7b-32768'}, {'age': 33, 'profession': 'actor', 'llm': 'gemma-7b-it'}, {'age': 30, 'profession': 'dancer', 'llm': 'llama-guard-3-8b'}, {'age': 32, 'profession': 'musician', 'llm': 'gemma-7b-it'}, {'age': 29, 'profession': 'actor', 'llm': 'llama-3.1-70b-versatile'}, {'age': 27, 'profession': 'artist', 'llm': 'gpt-4o'}, {'age': 33, 'profession': 'barber', 'llm': 'llama3-70b-8192'}, {'age': 28, 'profession': 'barber', 'llm': 'llama-3.1-8b-instant'}, {'age': 33, 'profession': 'dancer', 'llm': 'gpt-4o'}, {'age': 29, 'profession': 'dancer', 'llm': 'gemma-7b-it'}, {'age': 35, 'profession': 'writer', 'llm': 'llama-3.1-70b-versatile'}, {'age': 32, 'profession': 'doctor', 'llm': 'llama3-8b-8192'}, {'age': 26, 'profession': 'nurse', 'llm': 'llama3-70b-8192'}, {'age': 26, 'profession': 'musician', 'llm': 'gemma2-9b-it'}, {'age': 25, 'profession': 'lawyer', 'llm': 'llama-guard-3-8b'}, {'age': 31, 'profession': 'barber', 'llm': 'llama-3.1-8b-instant'}, {'age': 34, 'profession': 'writer', 'llm': 'llama3-70b-8192'}, {'age': 27, 'profession': 'dancer', 'llm': 'gpt-4o'}, {'age': 25, 'profession': 'chef', 'llm': 'llama3-70b-8192'}, {'age': 32, 'profession': 'engineer', 'llm': 'llama-3.1-8b-instant'}, {'age': 32, 'profession': 'doctor', 'llm': 'llama-3.1-70b-versatile'}, {'age': 26, 'profession': 'writer', 'llm': 'gemma-7b-it'}, {'age': 25, 'profession': 'actor', 'llm': 'gpt-4o'}, {'age': 35, 'profession': 'engineer', 'llm': 'gpt-4o'}, {'age': 25, 'profession': 'dancer', 'llm': 'mixtral-8x7b-32768'}, {'age': 31, 'profession': 'student', 'llm': 'gpt-4o'}, {'age': 28, 'profession': 'musician', 'llm': 'llama-3.1-70b-versatile'}, {'age': 33, 'profession': 'dancer', 'llm': 'gemma-7b-it'}, {'age': 34, 'profession': 'teacher', 'llm': 'llama3-70b-8192'}, {'age': 28, 'profession': 'engineer', 'llm': 'gemma2-9b-it'}, {'age': 29, 'profession': 'musician', 'llm': 'mixtral-8x7b-32768'}, {'age': 32, 'profession': 'dancer', 'llm': 'gemma-7b-it'}, {'age': 26, 'profession': 'teacher', 'llm': 'llama3-70b-8192'}, {'age': 26, 'profession': 'barber', 'llm': 'gemma2-9b-it'}, {'age': 28, 'profession': 'nurse', 'llm': 'llama3-70b-8192'}, {'age': 25, 'profession': 'nurse', 'llm': 'llama3-8b-8192'}, {'age': 29, 'profession': 'writer', 'llm': 'gpt-4o'}, {'age': 29, 'profession': 'scientist', 'llm': 'llama-3.1-8b-instant'}, {'age': 32, 'profession': 'student', 'llm': 'llama3-70b-8192'}, {'age': 31, 'profession': 'pilot', 'llm': 'gpt-4o'}, {'age': 26, 'profession': 'lawyer', 'llm': 'llama3-70b-8192'}, {'age': 29, 'profession': 'doctor', 'llm': 'gemma2-9b-it'}, {'age': 29, 'profession': 'chef', 'llm': 'llama-guard-3-8b'}, {'age': 25, 'profession': 'barber', 'llm': 'gemma-7b-it'}, {'age': 27, 'profession': 'engineer', 'llm': 'llama-3.1-70b-versatile'}, {'age': 26, 'profession': 'engineer', 'llm': 'llama3-8b-8192'}, {'age': 31, 'profession': 'doctor', 'llm': 'mixtral-8x7b-32768'}, {'age': 30, 'profession': 'artist', 'llm': 'gemma-7b-it'}, {'age': 34, 'profession': 'actor', 'llm': 'llama-guard-3-8b'}, {'age': 25, 'profession': 'lawyer', 'llm': 'llama-3.1-8b-instant'}, {'age': 35, 'profession': 'pilot', 'llm': 'llama-guard-3-8b'}, {'age': 33, 'profession': 'chef', 'llm': 'gemma2-9b-it'}, {'age': 33, 'profession': 'doctor', 'llm': 'llama-guard-3-8b'}, {'age': 32, 'profession': 'writer', 'llm': 'llama-3.1-8b-instant'}, {'age': 25, 'profession': 'dancer', 'llm': 'gpt-4o'}, {'age': 29, 'profession': 'nurse', 'llm': 'gemma2-9b-it'}, {'age': 33, 'profession': 'engineer', 'llm': 'gpt-4o'}, {'age': 32, 'profession': 'engineer', 'llm': 'llama-3.1-8b-instant'}, {'age': 25, 'profession': 'dancer', 'llm': 'llama3-70b-8192'}, {'age': 26, 'profession': 'doctor', 'llm': 'gemma-7b-it'}, {'age': 28, 'profession': 'artist', 'llm': 'llama3-70b-8192'}, {'age': 34, 'profession': 'nurse', 'llm': 'gemma-7b-it'}, {'age': 27, 'profession': 'doctor', 'llm': 'llama3-70b-8192'}, {'age': 28, 'profession': 'scientist', 'llm': 'llama3-8b-8192'}, {'age': 33, 'profession': 'chef', 'llm': 'mixtral-8x7b-32768'}, {'age': 33, 'profession': 'pilot', 'llm': 'gpt-4o'}, {'age': 30, 'profession': 'dancer', 'llm': 'mixtral-8x7b-32768'}, {'age': 35, 'profession': 'doctor', 'llm': 'gpt-4o'}, {'age': 31, 'profession': 'artist', 'llm': 'mixtral-8x7b-32768'}, {'age': 30, 'profession': 'barber', 'llm': 'gpt-4o'}, {'age': 27, 'profession': 'student', 'llm': 'mixtral-8x7b-32768'}, {'age': 30, 'profession': 'scientist', 'llm': 'llama3-8b-8192'}, {'age': 32, 'profession': 'barber', 'llm': 'llama-guard-3-8b'}, {'age': 31, 'profession': 'doctor', 'llm': 'gpt-4o'}, {'age': 30, 'profession': 'student', 'llm': 'mixtral-8x7b-32768'}, {'age': 31, 'profession': 'writer', 'llm': 'gemma2-9b-it'}, {'age': 25, 'profession': 'engineer', 'llm': 'gpt-4o'}, {'age': 29, 'profession': 'barber', 'llm': 'gemma2-9b-it'}, {'age': 25, 'profession': 'lawyer', 'llm': 'llama3-70b-8192'}, {'age': 35, 'profession': 'teacher', 'llm': 'gemma-7b-it'}, {'age': 27, 'profession': 'chef', 'llm': 'llama-guard-3-8b'}, {'age': 25, 'profession': 'scientist', 'llm': 'gemma2-9b-it'}, {'age': 27, 'profession': 'actor', 'llm': 'llama-3.1-70b-versatile'}, {'age': 30, 'profession': 'student', 'llm': 'gemma2-9b-it'}, {'age': 27, 'profession': 'barber', 'llm': 'llama3-70b-8192'}, {'age': 27, 'profession': 'actor', 'llm': 'llama3-8b-8192'}, {'age': 27, 'profession': 'dancer', 'llm': 'gpt-4o'}, {'age': 32, 'profession': 'lawyer', 'llm': 'llama3-8b-8192'}, {'age': 25, 'profession': 'barber', 'llm': 'llama-guard-3-8b'}, {'age': 29, 'profession': 'lawyer', 'llm': 'gemma-7b-it'}, {'age': 29, 'profession': 'chef', 'llm': 'llama-guard-3-8b'}, {'age': 31, 'profession': 'scientist', 'llm': 'llama3-8b-8192'}, {'age': 27, 'profession': 'scientist', 'llm': 'llama-3.1-70b-versatile'}, {'age': 25, 'profession': 'barber', 'llm': 'gemma-7b-it'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def gather_response(data):\n",
        "  final_output = []\n",
        "  count = 0\n",
        "  # for i in range(3):\n",
        "  #   audience = create_audience(30, age_category, PROFESSIONS)\n",
        "  for response in data['model_responses']:\n",
        "    model = response['model']\n",
        "    marketing_pitch = response['completion']\n",
        "    print(f\"Evaluating pitch for model: {model}\")\n",
        "    audience_responses = await get_audience_responses(audience, marketing_pitch)\n",
        "    # Analyze the results\n",
        "    true_count = sum(1 for response in audience_responses if response['response'] == True)\n",
        "    false_count = len(audience_responses) - true_count\n",
        "    # Output the results\n",
        "    print(f\"Number of people who would buy the product: {true_count}\")\n",
        "    print(f\"Number of people who would not buy the product: {false_count}\")\n",
        "    final_output.append(audience_responses)\n",
        "  return final_output\n",
        "    # # Optional: Print detailed responses\n",
        "    # for response in audience_responses:\n",
        "    #     print(f\"Age: {response['age']}, Profession: {response['profession']}, Would Buy: {response['response']}\")"
      ],
      "metadata": {
        "id": "yFmeyuz4dUzZ"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_output = asyncio.run(gather_response(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "CMi7hn6jfToQ",
        "outputId": "9d73c549-1b33-41b7-fe6a-2f7b8bb19944"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating pitch for model: llama3-8b-8192\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InternalServerError",
          "evalue": "Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-0769f6d18284>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgather_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_must_cancel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-84-442bd0d34443>\u001b[0m in \u001b[0;36mgather_response\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmarketing_pitch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'completion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Evaluating pitch for model: {model}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0maudience_responses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mget_audience_responses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarketing_pitch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Analyze the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrue_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maudience_responses\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-b3d6ad90dfab>\u001b[0m in \u001b[0;36mget_audience_responses\u001b[0;34m(audience, pitch)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_response_from_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAsyncGroq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GROQ_API_KEY_TIRTH\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Combine the results with audience details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;31m# This may also be a cancellation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-33acd9253561>\u001b[0m in \u001b[0;36mget_response_from_llm\u001b[0;34m(client, model, pitch, age, profession)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_response_from_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     response = await client.chat.completions.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         messages=[\n\u001b[1;32m      5\u001b[0m             {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    576\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \"\"\"\n\u001b[0;32m--> 578\u001b[0;31m         return await self._post(\n\u001b[0m\u001b[1;32m    579\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             body=await async_maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0masync_to_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         )\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m     async def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1476\u001b[0m         \u001b[0mremaining_retries\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m     ) -> ResponseT | _AsyncStreamT:\n\u001b[0;32m-> 1478\u001b[0;31m         return await self._request(\n\u001b[0m\u001b[1;32m   1479\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1569\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m         return await self._process_response(\n",
            "\u001b[0;31mInternalServerError\u001b[0m: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3opWgsTM0CJA",
        "outputId": "d6a9ca5b-13ee-4380-bd7e-b90f9085f2ae"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'age': 32, 'profession': 'nurse', 'llm': 'gemma-7b-it', 'response': True}, {'age': 27, 'profession': 'actor', 'llm': 'gemma-7b-it', 'response': True}, {'age': 33, 'profession': 'lawyer', 'llm': 'gemma2-9b-it', 'response': True}, {'age': 28, 'profession': 'barber', 'llm': 'gemma2-9b-it', 'response': False}, {'age': 30, 'profession': 'writer', 'llm': 'mixtral-8x7b-32768', 'response': True}, {'age': 31, 'profession': 'student', 'llm': 'gemma2-9b-it', 'response': False}, {'age': 30, 'profession': 'artist', 'llm': 'gemma-7b-it', 'response': True}, {'age': 31, 'profession': 'barber', 'llm': 'gemma-7b-it', 'response': True}, {'age': 30, 'profession': 'teacher', 'llm': 'mixtral-8x7b-32768', 'response': True}, {'age': 25, 'profession': 'barber', 'llm': 'mixtral-8x7b-32768', 'response': True}, {'age': 34, 'profession': 'barber', 'llm': 'mixtral-8x7b-32768', 'response': True}, {'age': 35, 'profession': 'student', 'llm': 'gemma2-9b-it', 'response': True}, {'age': 30, 'profession': 'student', 'llm': 'gemma-7b-it', 'response': True}, {'age': 29, 'profession': 'engineer', 'llm': 'gemma2-9b-it', 'response': True}, {'age': 30, 'profession': 'engineer', 'llm': 'mixtral-8x7b-32768', 'response': True}, {'age': 33, 'profession': 'musician', 'llm': 'gemma-7b-it', 'response': True}, {'age': 33, 'profession': 'student', 'llm': 'gemma-7b-it', 'response': True}, {'age': 31, 'profession': 'writer', 'llm': 'mixtral-8x7b-32768', 'response': False}, {'age': 31, 'profession': 'engineer', 'llm': 'mixtral-8x7b-32768', 'response': True}, {'age': 32, 'profession': 'writer', 'llm': 'mixtral-8x7b-32768', 'response': True}, {'age': 35, 'profession': 'dancer', 'llm': 'gemma-7b-it', 'response': True}, {'age': 33, 'profession': 'teacher', 'llm': 'gemma-7b-it', 'response': True}, {'age': 31, 'profession': 'barber', 'llm': 'gemma2-9b-it', 'response': False}, {'age': 35, 'profession': 'chef', 'llm': 'mixtral-8x7b-32768', 'response': True}, {'age': 30, 'profession': 'scientist', 'llm': 'gemma-7b-it', 'response': True}, {'age': 26, 'profession': 'doctor', 'llm': 'gemma-7b-it', 'response': True}, {'age': 30, 'profession': 'teacher', 'llm': 'gemma-7b-it', 'response': True}, {'age': 33, 'profession': 'barber', 'llm': 'gemma2-9b-it', 'response': True}, {'age': 30, 'profession': 'student', 'llm': 'mixtral-8x7b-32768', 'response': True}, {'age': 26, 'profession': 'actor', 'llm': 'gemma-7b-it', 'response': True}]]\n"
          ]
        }
      ]
    }
  ]
}